# Fine-Tune-FLAN-T5-with-Reinforcement-Learning-PPO-and-PEFT-to-Generate-Less-Toxic-Summaries
In this notebook, the FLAN-T5 model will be fine-tuned to generate less toxic content using Meta AI's hate speech reward model. The reward model classifies text as either "not hate" or "hate." Proximal Policy Optimization (PPO) will be used to fine-tune the model and reduce its toxicity.
-- Done as a part of the "Genarative AI with LLMs" course by AWS and Coursera --
