{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"instance_type":"ml.m5.2xlarge","kernelspec":{"display_name":"Python 3 (Data Science 3.0)","language":"python","name":"python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{"tags":[]}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel and Required Dependencies](#1)\n- [ 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator](#2)\n  - [ 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction](#2.1)\n  - [ 2.2 - Prepare Reward Model](#2.2)\n  - [ 2.3 - Evaluate Toxicity](#2.3)\n- [ 3 - Perform Fine-Tuning to Detoxify the Summaries](#3)\n  - [ 3.1 - Initialize `PPOTrainer`](#3.1)\n  - [ 3.2 - Fine-Tune the Model](#3.2)\n  - [ 3.3 - Evaluate the Model Quantitatively](#3.3)\n  - [ 3.4 - Evaluate the Model Qualitatively](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel and Required Dependencies","metadata":{"tags":[]}},{"cell_type":"markdown","source":"First, check that the correct kernel is chosen.\n\n<img src=\"images/kernel_set_up.png\" width=\"300\"/>\n\nYou can click on that (top right of the screen) to see and check the details of the image, kernel, and instance type.\n\n<img src=\"images/w3_kernel_and_instance_type.png\" width=\"600\"/>\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik01MCw5NyBBNTAsNTAgMCAwIDEgNTMsMyBMNzk3LCAzIEw3OTcsOTcgTDUwLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPHRleHQgeD0iMTAwIiB5PSIzNCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5QbGVhc2UgbWFrZSBzdXJlIHRoYXQgeW91IGNob29zZTwvdGV4dD4gCiAgICA8dGV4dCB4PSIzMjAiIHk9IjM0IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTQiIGZpbGw9IiMzMzMzMzMiIGZvbnQtd2VpZ2h0PSJib2xkIj5tbC5tNS4yeGxhcmdlPC90ZXh0PgogICAgPHRleHQgeD0iNDE4IiB5PSIzNCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5pbnN0YW5jZSB0eXBlLjwvdGV4dD4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iNTYiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VG8gZmluZCB0aGF0IGluc3RhbmNlIHR5cGUsIHlvdSBtaWdodCBoYXZlIHRvIHNjcm9sbCBkb3duIHRvIHRoZSAiQWxsIEluc3RhbmNlcyIgc2VjdGlvbiBpbiB0aGUgZHJvcGRvd24uPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI3OCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5DaG9pY2Ugb2YgYW5vdGhlciBpbnN0YW5jZSB0eXBlIG1pZ2h0IGNhdXNlIHRyYWluaW5nIGZhaWx1cmUva2VybmVsIGhhbHQvYWNjb3VudCBkZWFjdGl2YXRpb24uPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"import os\n\ninstance_type_expected = 'ml-m5-2xlarge'\ninstance_type_current = os.environ.get('HOSTNAME')\n\nprint(f'Expected instance type: instance-datascience-{instance_type_expected}')\nprint(f'Currently chosen instance type: {instance_type_current}')\n\nassert instance_type_expected in instance_type_current, f'ERROR. You selected the {instance_type_current} instance type. Please select {instance_type_expected} instead as shown on the screenshot above'\nprint(\"Instance type has been chosen correctly.\")","metadata":{"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Expected instance type: instance-datascience-ml-m5-2xlarge\n\nCurrently chosen instance type: instance-datascience-ml-m5-2xlarge\n\nInstance type has been chosen correctly.\n"}]},{"cell_type":"markdown","source":"Now install the required packages to use PyTorch and Hugging Face transformers and datasets.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"%pip install -U datasets==2.17.0\n\n%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    peft==0.3.0 --quiet\n\n# Installing the Reinforcement Learning library directly from github.\n%pip install git+https://github.com/lvwerra/trl.git@25fa1bd    ","metadata":{"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting datasets==2.17.0\n\n  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.13.4)\n\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.4)\n\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (15.0.2)\n\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\n\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.2.2)\n\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.1)\n\nCollecting xxhash (from datasets==2.17.0)\n\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\n\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n\nCollecting aiohttp (from datasets==2.17.0)\n\n  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n\nCollecting huggingface-hub>=0.19.4 (from datasets==2.17.0)\n\n  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\n\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.17.0)\n\n  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.17.0)\n\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.17.0)\n\n  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n\nCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.17.0)\n\n  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n\nCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.17.0)\n\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.11.0)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.2.1)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.2.2)\n\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.9.0)\n\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n\nDownloading datasets-2.17.0-py3-none-any.whl (536 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n\nDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hInstalling collected packages: xxhash, multidict, fsspec, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n\n  Attempting uninstall: fsspec\n\n    Found existing installation: fsspec 2024.3.1\n\n    Uninstalling fsspec-2024.3.1:\n\n      Successfully uninstalled fsspec-2024.3.1\n\nSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.17.0 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.23.4 multidict-6.0.5 xxhash-3.4.1 yarl-1.9.4\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n\nCollecting pip\n\n  Downloading pip-24.1-py3-none-any.whl.metadata (3.6 kB)\n\nDownloading pip-24.1-py3-none-any.whl (1.8 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hInstalling collected packages: pip\n\n  Attempting uninstall: pip\n\n    Found existing installation: pip 24.0\n\n    Uninstalling pip-24.0:\n\n      Successfully uninstalled pip-24.0\n\nSuccessfully installed pip-24.1\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\nCollecting git+https://github.com/lvwerra/trl.git@25fa1bd\n\n  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-25c94424\n\n  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-25c94424\n\n\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n\n\u001b[0m  Running command git checkout -q 25fa1bd\n\n  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.13.1)\n\nRequirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (4.27.2)\n\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.26.4)\n\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (0.31.0)\n\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (2.17.0)\n\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.11.0)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (8.5.0.96)\n\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.10.3.66)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (69.5.1)\n\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (0.43.0)\n\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.13.4)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.23.4)\n\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2023.12.25)\n\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\n\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.66.1)\n\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (5.9.8)\n\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (0.4.3)\n\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (15.0.2)\n\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.6)\n\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.3.8)\n\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (2.2.2)\n\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.70.16)\n\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->trl==0.4.2.dev0) (2023.10.0)\n\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.9.5)\n\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.2.0)\n\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.1)\n\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.5)\n\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.4)\n\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.2.1)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2024.2.2)\n\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.9.0)\n\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2024.1)\n\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2024.1)\n\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n\nBuilding wheels for collected packages: trl\n\n  Building wheel for trl (setup.py) ... \u001b[?25ldone\n\n\u001b[?25h  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67534 sha256=c375bfac1d9859c100b1ac542dc5e94a07e23a290cc429746ea36150fb870f06\n\n  Stored in directory: /tmp/pip-ephem-wheel-cache-yf3one0a/wheels/24/b4/20/2fa3a1e47c0411c39e198029315e3af2a2c1d59132913f136f\n\nSuccessfully built trl\n\nInstalling collected packages: trl\n\nSuccessfully installed trl-0.4.2.dev0\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\nfrom datasets import load_dataset\nfrom peft import PeftModel, PeftConfig, LoraConfig, TaskType\n\n# trl: Transformer Reinforcement Learning library\nfrom trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\nfrom trl import create_reference_model\nfrom trl.core import LengthSampler\n\nimport torch\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\n\n# tqdm library makes the loops show a smart progress meter.\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"tags":[]},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator","metadata":{}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction","metadata":{"tags":[]}},{"cell_type":"markdown","source":"You will keep working with the same Hugging Face dataset [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) and the pre-trained model [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5). ","metadata":{"tags":[]}},{"cell_type":"code","source":"model_name=\"google/flan-t5-base\"\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset_original = load_dataset(huggingface_dataset_name)\n\ndataset_original","metadata":{"tags":[]},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"940be41f7de046a9a68234b328f0b032","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c9fabfb812b4d8e8f38c7383c153ca0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"027d01bf384144aaa23b6faab6fdc3b3","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13b77c7434ac405882bed063ee3abbad","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f30c9dd3f729486ea435e724cff28c41","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ba4b5c3384f4a25811cf95543f02f5e","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"785ab0f8283c45d0bef710f2f1a970d4","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 12460\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 500\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1500\n","    })\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"The next step will be to preprocess the dataset. You will take only a part of it, then filter the dialogues of a particular length (just to make those examples long enough and, at the same time, easy to read). Then wrap each dialogue with the instruction and tokenize the prompts. Save the token ids in the field `input_ids` and decoded version of the prompts in the field `query`.\n\nYou could do that all step by step in the cell below, but it is a good habit to organize that all in a function `build_dataset`:","metadata":{}},{"cell_type":"code","source":"def build_dataset(model_name,\n                  dataset_name,\n                  input_min_text_length, \n                  input_max_text_length):\n\n    \"\"\"\n    Preprocess the dataset and split it into train and test parts.\n\n    Parameters:\n    - model_name (str): Tokenizer model name.\n    - dataset_name (str): Name of the dataset to load.\n    - input_min_text_length (int): Minimum length of the dialogues.\n    - input_max_text_length (int): Maximum length of the dialogues.\n        \n    Returns:\n    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n    \"\"\"\n    \n    # load dataset (only \"train\" part will be enough for this lab).\n    dataset = load_dataset(dataset_name, split=\"train\")\n    \n    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n\n    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n    \n    def tokenize(sample):\n        \n        # Wrap each dialogue with the instruction.\n        prompt = f\"\"\"\nSummarize the following conversation.\n\n{sample[\"dialogue\"]}\n\nSummary:\n\"\"\"\n        sample[\"input_ids\"] = tokenizer.encode(prompt)\n        \n        # This must be called \"query\", which is a requirement of our PPO library.\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    # Tokenize each dialogue.\n    dataset = dataset.map(tokenize, batched=False)\n    dataset.set_format(type=\"torch\")\n    \n    # Split the dataset into train and test parts.\n    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n\n    return dataset_splits\n\ndataset = build_dataset(model_name=model_name,\n                        dataset_name=huggingface_dataset_name,\n                        input_min_text_length=200, \n                        input_max_text_length=1000)\n\nprint(dataset)","metadata":{"tags":[]},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5685d4a7825446ed8883d8563e88bc12","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef84c01ff55541109a90f1152a44ac0b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"baeddd764cad46a98d34c747796a9b68","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fb1ab597c8f48879a6c4da83a7893e9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22dfb867ddc746c392c2624e7812e2d1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"859a73c0015f48be8767e9cc168a75a2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"DatasetDict({\n\n    train: Dataset({\n\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n\n        num_rows: 8017\n\n    })\n\n    test: Dataset({\n\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n\n        num_rows: 2005\n\n    })\n\n})\n"}]},{"cell_type":"markdown","source":"In the previous lab, you fine-tuned the PEFT model with summarization instructions. The training in the notebook was done on a subset of data. Then you downloaded the checkpoint of the fully trained PEFT model from S3. \n\nLet's load the same model checkpoint here:","metadata":{"tags":[]}},{"cell_type":"code","source":"!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n\ndownload: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"}]},{"cell_type":"markdown","source":"List the model item and check its size (it's less than 15 Mb):","metadata":{"tags":[]}},{"cell_type":"code","source":"!ls -alh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n-rw-r--r-- 1 root root 14M May 15  2023 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"}]},{"cell_type":"markdown","source":"Prepare a function to pull out the number of model parameters (it is the same as in the previous lab):","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"","metadata":{"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Add the adapter to the original FLAN-T5 model. In the previous lab you were adding the fully trained adapter only for inferences, so there was no need to pass LoRA configurations doing that. Now you need to pass them to the constructed PEFT model, also putting `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n                                              torch_dtype=torch.bfloat16)\n\npeft_model = PeftModel.from_pretrained(model, \n                                       './peft-dialogue-summary-checkpoint-from-s3/', \n                                       lora_config=lora_config,\n                                       torch_dtype=torch.bfloat16, \n                                       device_map=\"auto\",                                       \n                                       is_trainable=True)\n\nprint(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n","metadata":{"tags":[]},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"765f1ba371c749e1b4863ad483e0d12c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1f625c3bb3e44bb9b58586491c7ff45","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a466a0f856774318ad4a1d0e81ca93a3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"PEFT model parameters to be updated:\n\n\n\ntrainable model parameters: 3538944\n\nall model parameters: 251116800\n\npercentage of trainable model parameters: 1.41%\n\n\n"}]},{"cell_type":"markdown","source":"In this lab, you are preparing to fine-tune the LLM using Reinforcement Learning (RL). RL will be briefly discussed in the next section of this lab, but at this stage, you just need to prepare the Proximal Policy Optimization (PPO) model passing the instruct-fine-tuned PEFT model to it. PPO will be used to optimize the RL policy against the reward model.","metadata":{}},{"cell_type":"code","source":"ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n                                                               torch_dtype=torch.bfloat16,\n                                                               is_trainable=True)\n\nprint(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\nprint(ppo_model.v_head)","metadata":{"tags":[]},"execution_count":11,"outputs":[{"name":"stderr","output_type":"stream","text":"Detected kernel version 4.14.344, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"},{"name":"stdout","output_type":"stream","text":"PPO model parameters to be updated (ValueHead + 769 params):\n\n\n\ntrainable model parameters: 3539713\n\nall model parameters: 251117569\n\npercentage of trainable model parameters: 1.41%\n\n\n\nValueHead(\n\n  (dropout): Dropout(p=0.1, inplace=False)\n\n  (summary): Linear(in_features=768, out_features=1, bias=True)\n\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n\n)\n"}]},{"cell_type":"markdown","source":"During PPO, only a few parameters will be updated. Specifically, the parameters of the `ValueHead`. More information about this class of models can be found in the [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters can be computed as $(n+1)*m$, where $n$ is the number of input units (here $n=768$) and $m$ is the number of output units (you have $m=1$). The $+1$ term in the equation takes into account the bias term.","metadata":{}},{"cell_type":"markdown","source":"Now create a frozen copy of the PPO which will not be fine-tuned - a reference model. The reference model will represent the LLM before detoxification. None of the parameters of the reference model will be updated during PPO training. This is on purpose.","metadata":{}},{"cell_type":"code","source":"ref_model = create_reference_model(ppo_model)\n\nprint(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')","metadata":{"tags":[]},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"Reference model parameters to be updated:\n\n\n\ntrainable model parameters: 0\n\nall model parameters: 251117569\n\npercentage of trainable model parameters: 0.00%\n\n\n"}]},{"cell_type":"markdown","source":"Everything is set. It is time to prepare the reward model!","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Prepare Reward Model\n\n**Reinforcement Learning (RL)** is one type of machine learning where agents take actions in an environment aimed at maximizing their cumulative rewards. The agent's behavior is defined by the **policy**. And the goal of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the **reward function**. \n\nIn the [previous section](#2.1) the original policy is based on the instruct PEFT model - this is the LLM before detoxification. Then you could ask human labelers to give feedback on the outputs' toxicity. However, it can be expensive to use them for the entire fine-tuning process. A practical way to avoid that is to use a reward model encouraging the agent to detoxify the dialogue summaries. The intuitive approach would be to do some form of sentiment analysis across two classes (`nothate` and `hate`) and give a higher reward if there is higher a chance of getting class `nothate` as an output. \n\nFor example, we can mention that having human labelers for the entire finetuning process can be expensive. A practical way to avoid that is to use a reward model.\n\nuse feedback generated by a model\n\nYou will use [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) for the reward model. This model will output **logits** and then predict probabilities across two classes: `nothate` and `hate`. The logits of the output `nothate` will be taken as a positive reward. Then, the model will be fine-tuned with PPO using those reward values.\n\nCreate the instance of the required model class for the RoBERTa model. You also need to load a tokenizer to test the model. Notice that the model label `0` will correspond to the class `nothate` and label `1` to the class `hate`.","metadata":{}},{"cell_type":"code","source":"toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\ntoxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\ntoxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\nprint(toxicity_model.config.id2label)","metadata":{"tags":[]},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1db306807c2b47e984688f2a940d6da5","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18ab2523080d43dba6e319a9a5c1d727","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b95602d8579e427e8a009c4aeaab088f","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e2cb75ab5914bb19d2834c7d3047146","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"961e737d12d2446bb904b55ace3b581c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac737c8a38bd4ee6a178450ae37668df","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"{0: 'nothate', 1: 'hate'}\n"}]},{"cell_type":"markdown","source":"Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning.","metadata":{"tags":[]}},{"cell_type":"code","source":"non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n\ntoxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n\nlogits = toxicity_model(input_ids=toxicity_input_ids).logits\nprint(f'logits [not hate, hate]: {logits.tolist()[0]}')\n\n# Print the probabilities for [not hate, hate]\nprobabilities = logits.softmax(dim=-1).tolist()[0]\nprint(f'probabilities [not hate, hate]: {probabilities}')\n\n# get the logits for \"not hate\" - this is the reward!\nnot_hate_index = 0\nnothate_reward = (logits[:, not_hate_index]).tolist()\nprint(f'reward (high): {nothate_reward}')","metadata":{"tags":[]},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"logits [not hate, hate]: [3.114100694656372, -2.4896175861358643]\n\nprobabilities [not hate, hate]: [0.9963293671607971, 0.003670616541057825]\n\nreward (high): [3.114100694656372]\n"}]},{"cell_type":"markdown","source":"Let's show a toxic comment.  This will have a low reward because it is more toxic.","metadata":{}},{"cell_type":"code","source":"toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n\ntoxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n\nlogits = toxicity_model(toxicity_input_ids).logits\nprint(f'logits [not hate, hate]: {logits.tolist()[0]}')\n\n# Print the probabilities for [not hate, hate]\nprobabilities = logits.softmax(dim=-1).tolist()[0]\nprint(f'probabilities [not hate, hate]: {probabilities}')\n\n# Get the logits for \"not hate\" - this is the reward!\nnothate_reward = (logits[:, not_hate_index]).tolist() \nprint(f'reward (low): {nothate_reward}')","metadata":{"tags":[]},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"logits [not hate, hate]: [-0.6921188831329346, 0.3722729980945587]\n\nprobabilities [not hate, hate]: [0.25647106766700745, 0.7435289621353149]\n\nreward (low): [-0.6921188831329346]\n"}]},{"cell_type":"markdown","source":"Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:","metadata":{}},{"cell_type":"code","source":"device = 0 if torch.cuda.is_available() else \"cpu\"\n\nsentiment_pipe = pipeline(\"sentiment-analysis\", \n                          model=toxicity_model_name, \n                          device=device)\nreward_logits_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n    \"batch_size\": 16\n}\n\nreward_probabilities_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n    \"batch_size\": 16\n}\n\nprint(\"Reward model output:\")\nprint(\"For non-toxic text\")\nprint(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\nprint(\"For toxic text\")\nprint(sentiment_pipe(toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[]},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Reward model output:\n\nFor non-toxic text\n\n[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n\n[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n\nFor toxic text\n\n[{'label': 'hate', 'score': 0.3722729980945587}, {'label': 'nothate', 'score': -0.6921188831329346}]\n\n[{'label': 'hate', 'score': 0.7435289621353149}, {'label': 'nothate', 'score': 0.25647106766700745}]\n"}]},{"cell_type":"markdown","source":"The outputs are the logits for both `nothate` (positive) and `hate` (negative) classes. But PPO will be using logits only of the `nothate` class as the positive reward signal used to help detoxify the LLM outputs.","metadata":{}},{"cell_type":"code","source":"print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[]},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n\n[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n"}]},{"cell_type":"code","source":"print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"[{'label': 'hate', 'score': 0.3722729980945587}, {'label': 'nothate', 'score': -0.6921188831329346}]\n\n[{'label': 'hate', 'score': 0.7435289621353149}, {'label': 'nothate', 'score': 0.25647106766700745}]\n"}]},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate Toxicity\n\nTo evaluate the model before and after fine-tuning/detoxification you need to set up the [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity). The **toxicity score** is a decimal value between 0 and 1 where 1 is the highest toxicity.","metadata":{"tags":[]}},{"cell_type":"code","source":"toxicity_evaluator = evaluate.load(\"toxicity\", \n                                    toxicity_model_name,\n                                    module_type=\"measurement\",\n                                    toxic_label=\"hate\")","metadata":{"tags":[]},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc991e148b424b5a87f44d8c95381911","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"Try to calculate toxicity for the same sentences as in section [2.2](#2.2). It's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model.","metadata":{"tags":[]}},{"cell_type":"code","source":"toxicity_score = toxicity_evaluator.compute(predictions=[\n    non_toxic_text\n])\n\nprint(\"Toxicity score for non-toxic text:\")\nprint(toxicity_score[\"toxicity\"])\n\ntoxicity_score = toxicity_evaluator.compute(predictions=[\n    toxic_text\n])\n\nprint(\"\\nToxicity score for toxic text:\")\nprint(toxicity_score[\"toxicity\"])","metadata":{"tags":[]},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"Toxicity score for non-toxic text:\n\n[0.003670616541057825]\n\n\n\nToxicity score for toxic text:\n\n[0.7435289621353149]\n"}]},{"cell_type":"markdown","source":"This evaluator can be used to compute the toxicity of the dialogues prepared in section [2.1](#2.1). You will need to pass the test dataset (`dataset[\"test\"]`), the same tokenizer which was used in that section, the frozen PEFT model prepared in section [2.2](#2.2), and the toxicity evaluator. It is convenient to wrap the required steps in the function `evaluate_toxicity`. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def evaluate_toxicity(model, \n                      toxicity_evaluator, \n                      tokenizer, \n                      dataset, \n                      num_samples):\n    \n    \"\"\"\n    Preprocess the dataset and split it into train and test parts.\n\n    Parameters:\n    - model (trl model): Model to be evaluated.\n    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n    - tokenizer (transformers tokenizer): Tokenizer to be used.\n    - dataset (dataset): Input dataset for the evaluation.\n    - num_samples (int): Maximum number of samples for the evaluation.\n        \n    Returns:\n    tuple: A tuple containing two numpy.float64 values:\n    - mean (numpy.float64): Mean of the samples toxicity.\n    - std (numpy.float64): Standard deviation of the samples toxicity.\n    \"\"\"\n\n    max_new_tokens=100\n\n    toxicities = []\n    input_texts = []\n    for i, sample in tqdm(enumerate(dataset)):\n        input_text = sample[\"query\"]\n\n        if i > num_samples:\n            break\n            \n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n        \n        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n                                             top_k=0.0,\n                                             top_p=1.0,\n                                             do_sample=True)\n\n        response_token_ids = model.generate(input_ids=input_ids,\n                                            generation_config=generation_config)\n        \n        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n        \n        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n\n        toxicities.extend(toxicity_score[\"toxicity\"])\n\n    # Compute mean & std using np.\n    mean = np.mean(toxicities)\n    std = np.std(toxicities)\n        \n    return mean, std","metadata":{"tags":[]},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"And now perform the calculation of the model toxicity before fine-tuning/detoxification:","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n\nmean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n                                                                          toxicity_evaluator=toxicity_evaluator, \n                                                                          tokenizer=tokenizer, \n                                                                          dataset=dataset[\"test\"], \n                                                                          num_samples=10)\n\nprint(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')","metadata":{"tags":[]},"execution_count":22,"outputs":[{"name":"stderr","output_type":"stream","text":"11it [00:24,  2.23s/it]"},{"name":"stdout","output_type":"stream","text":"toxicity [mean, std] before detox: [0.020201776959848674, 0.020007159960385528]\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Fine-Tuning to Detoxify the Summaries\nOptimize a RL policy against the reward model using Proximal Policy Optimization (PPO).","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Initialize `PPOTrainer`\n \nFor the `PPOTrainer` initialization, you will need a collator. Here it will be a function transforming the dictionaries in a particular way. You can define and test it:","metadata":{}},{"cell_type":"code","source":"def collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\ntest_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\nprint(f'Collator input: {test_data}')\nprint(f'Collator output: {collator(test_data)}')","metadata":{"tags":[]},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n\nCollator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"}]},{"cell_type":"markdown","source":"Set up the configuration parameters. Load the `ppo_model` and the tokenizer. You will also load a frozen version of the model `ref_model`. The first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This works as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original LLM.","metadata":{}},{"cell_type":"code","source":"learning_rate=1.41e-5\nmax_ppo_epochs=1\nmini_batch_size=4\nbatch_size=16\n\nconfig = PPOConfig(\n    model_name=model_name,    \n    learning_rate=learning_rate,\n    ppo_epochs=max_ppo_epochs,\n    mini_batch_size=mini_batch_size,\n    batch_size=batch_size\n)\n\nppo_trainer = PPOTrainer(config=config, \n                         model=ppo_model, \n                         ref_model=ref_model, \n                         tokenizer=tokenizer, \n                         dataset=dataset[\"train\"], \n                         data_collator=collator)","metadata":{"tags":[]},"execution_count":24,"outputs":[{"name":"stderr","output_type":"stream","text":"Detected kernel version 4.14.344, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"}]},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Fine-Tune the Model","metadata":{}},{"cell_type":"markdown","source":"The fine-tuning loop consists of the following main steps:\n1. Get the query responses from the policy LLM (PEFT model).\n2. Get sentiments for query/responses from hate speech RoBERTa model.\n3. Optimize policy with PPO using the (query, response, reward) triplet.\n\nThe operation is running if you see the following metrics appearing:\n* `objective/kl`: minimize kl divergence,\n* `ppo/returns/mean`: maximize mean returns,\n* `ppo/policy/advantages_mean`: maximize advantages.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSAyMC0zMCBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"output_min_length = 100\noutput_max_length = 400\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\ngeneration_kwargs = {\n    \"min_length\": 5,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True\n}\n\nreward_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n    \"batch_size\": 16\n}\n\nmax_ppo_steps = 10\n\nfor step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    # Break when you reach max_steps.\n    if step >= max_ppo_steps:\n        break   \n\n    prompt_tensors = batch[\"input_ids\"]\n\n    # Get response from FLAN-T5/PEFT LLM.\n    summary_tensors = []\n\n    for prompt_tensor in prompt_tensors:\n        max_new_tokens = output_length_sampler()        \n            \n        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n        \n        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n        \n    # This needs to be called \"response\".\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n\n    # Compute reward outputs.\n    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n\n    # You use the `nothate` item because this is the score for the positive `nothate` class.\n    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n\n    # Run PPO step.\n    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n    ppo_trainer.log_stats(stats, batch, reward_tensors)\n    \n    print(f'objective/kl: {stats[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n    print('-'.join('' for x in range(100)))","metadata":{"tags":[]},"execution_count":25,"outputs":[{"name":"stderr","output_type":"stream","text":"0it [00:00, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n1it [01:44, 104.18s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 29.314075469970703\n\nppo/returns/mean: -0.8752288818359375\n\nppo/policy/advantages_mean: -9.5703862612595e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"2it [03:22, 100.47s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 33.91984558105469\n\nppo/returns/mean: -1.0599122047424316\n\nppo/policy/advantages_mean: -1.240169034133487e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"3it [04:51, 95.59s/it] "},{"name":"stdout","output_type":"stream","text":"objective/kl: 25.813386917114258\n\nppo/returns/mean: -0.7667686343193054\n\nppo/policy/advantages_mean: 1.09158282413091e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"4it [06:22, 93.64s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 27.114152908325195\n\nppo/returns/mean: -0.7416901588439941\n\nppo/policy/advantages_mean: -2.0798534450250372e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"5it [07:47, 90.35s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 24.75006866455078\n\nppo/returns/mean: -0.4672226905822754\n\nppo/policy/advantages_mean: -1.520284342859668e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"6it [09:26, 93.35s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 31.863704681396484\n\nppo/returns/mean: -1.0559757947921753\n\nppo/policy/advantages_mean: 4.8790615991833874e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"7it [10:56, 92.21s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 30.007814407348633\n\nppo/returns/mean: -0.9628568887710571\n\nppo/policy/advantages_mean: -4.2431267388565175e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"8it [12:27, 91.85s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 30.56298828125\n\nppo/returns/mean: -0.976753830909729\n\nppo/policy/advantages_mean: 5.599552821422549e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"9it [14:02, 92.92s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 30.089706420898438\n\nppo/returns/mean: -0.9358670711517334\n\nppo/policy/advantages_mean: -9.761196295698937e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"10it [15:38, 93.89s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 28.405818939208984\n\nppo/returns/mean: -0.8363462686538696\n\nppo/policy/advantages_mean: -1.154189854801757e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{}},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Quantitatively\n\nLoad the PPO/PEFT model back in from disk and use the test dataset split to evaluate the toxicity score of the RL-fine-tuned model.","metadata":{}},{"cell_type":"code","source":"mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n                                                                        toxicity_evaluator=toxicity_evaluator, \n                                                                        tokenizer=tokenizer, \n                                                                        dataset=dataset[\"test\"], \n                                                                        num_samples=10)\nprint(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')","metadata":{"tags":[]},"execution_count":26,"outputs":[{"name":"stderr","output_type":"stream","text":"11it [00:19,  1.74s/it]"},{"name":"stdout","output_type":"stream","text":"toxicity [mean, std] after detox: [0.03178686888846145, 0.03587621944345149]\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"And compare the toxicity scores of the reference model (before detoxification) and fine-tuned model (after detoxification).","metadata":{"tags":[]}},{"cell_type":"code","source":"mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\nstd_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n\nprint(f'Percentage improvement of toxicity score after detoxification:')\nprint(f'mean: {mean_improvement*100:.2f}%')\nprint(f'std: {std_improvement*100:.2f}%')","metadata":{"tags":[]},"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":"Percentage improvement of toxicity score after detoxification:\n\nmean: -57.35%\n\nstd: -79.32%\n"}]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Qualitatively\n\nLet's inspect some examples from the test dataset. You can compare the original `ref_model` to the fine-tuned/detoxified `ppo_model` using the toxicity evaluator.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSAyLTMgbWludXRlcyB0byBydW4uPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>\n​","metadata":{}},{"cell_type":"code","source":"batch_size = 20\ncompare_results = {}\n\ndf_batch = dataset[\"test\"][0:batch_size]\n\ncompare_results[\"query\"] = df_batch[\"query\"]\nprompt_tensors = df_batch[\"input_ids\"]\n\nsummary_tensors_ref = []\nsummary_tensors = []\n\n# Get response from ppo and base model.\nfor i in tqdm(range(batch_size)):\n    gen_len = output_length_sampler()\n    generation_kwargs[\"max_new_tokens\"] = gen_len\n    \n    summary = ref_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    summary_tensors_ref.append(summary)\n\n    summary = ppo_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    summary_tensors.append(summary)\n\n# Decode responses.\ncompare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\ncompare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n\n# Sentiment analysis of query/response pairs before/after.\ntexts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\nrewards_before = sentiment_pipe(texts_before, **reward_kwargs)\ncompare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n\ntexts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\nrewards_after = sentiment_pipe(texts_after, **reward_kwargs)\ncompare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]","metadata":{"tags":[]},"execution_count":28,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 20/20 [01:22<00:00,  4.13s/it]\n"}]},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Store and review the results in a DataFrame","metadata":{"tags":[]}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 500)\ndf_compare_results = pd.DataFrame(compare_results)\ndf_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\ndf_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\ndf_compare_results_sorted","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the reward mean/median of the generated sequences you can observe a significant difference!","metadata":{}}]}